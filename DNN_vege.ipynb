{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.15</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import pixiedust\n",
    "import pdb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed\n",
    "                  )\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#PREPARING FOR TENSORBOARD:\n",
    "from datetime import datetime\n",
    "#FOR DIFFERENT MODELS MAKE DIFFERENT LOG NAME!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/DNN_simple-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPARATION\n",
      "length of the fixed array is  3651826\n",
      "test :  [305.0, 162.0, 188.0, 184.0, 114.0, 438.0, 738.0, 698.0]\n",
      "number of value arrays is:  3651826  and number of classes is:  3651826\n"
     ]
    }
   ],
   "source": [
    "# %%pixie_debugger\n",
    "print ('DATA PREPARATION')\n",
    "vege_csv = csv.reader(open('merged1301_clean.csv', newline=''), delimiter=' ', quotechar='|')\n",
    "\n",
    "vege_array = []\n",
    "\n",
    "for row in vege_csv:\n",
    "\tred_array = []\n",
    "\titems = row[0].split(';')\n",
    "\tfor i in items:\n",
    "\t\tred_array.append(i)\n",
    "\n",
    "\tvege_array.append(red_array)\n",
    "\n",
    "klase = []\n",
    "values = []\n",
    "\n",
    "vege_array_fix = vege_array[:-4]\n",
    "\n",
    "print ('length of the fixed array is ', len (vege_array_fix))\n",
    "\n",
    "for r in vege_array_fix:\n",
    "    value_row = []\n",
    "    i = 0\n",
    "    while i < len(r):\n",
    "        if i == 0:\n",
    "#             klase.append(float(r[i]))\n",
    "            \n",
    "            if r[i] == '0':\n",
    "                klase.append(0)\n",
    "            if r[i] == '1':\n",
    "                klase.append(1)\n",
    "            if r[i] == '2':\n",
    "                klase.append(2)\n",
    "        else:\n",
    "            value_row.append(float(r[i]))\n",
    "        i = i + 1\n",
    "    values.append(value_row)\n",
    "\n",
    "print ('test : ', values[10000])\n",
    "print ('number of value arrays is: ', len(values), ' and number of classes is: ', len(klase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(values, klase, test_size=0.20, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = np.array(X_train) #.astype('float32')\n",
    "X_train = scaler.fit_transform(X_train.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n",
    "        activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.variable_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 8\n",
    "n_outputs = 3\n",
    "height = 1\n",
    "width = 1\n",
    "\n",
    "#not sure if this is right way to define inputs if I would like to make\n",
    "#pixel based classification model\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(np.float32, shape=[None, n_channels], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, n_channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, np.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validlen = int(len(X_train) * 0.1)\n",
    "\n",
    "X_valid = X_train[:validlen]\n",
    "X_train = X_train[validlen:]\n",
    "y_valid = y_train[:validlen]\n",
    "y_train = y_train[validlen:]\n",
    "#valied length is used primarily for implementing early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 500\n",
    "# batch_size = 5000\n",
    "\n",
    "# max_checks_without_progress = 200\n",
    "# checks_without_progress = 0\n",
    "# best_loss = np.infty\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     init.run()\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         rnd_idx = np.random.permutation(len(X_train))\n",
    "#         for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_size):\n",
    "# #             X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]\n",
    "#             X_batch = []\n",
    "#             y_batch = []\n",
    "#             for i in rnd_indices:\n",
    "#                 X_batch.append(X_train[i])\n",
    "#                 y_batch.append(y_train[i])\n",
    "#             sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "#         loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid, y: y_valid})\n",
    "#         if loss_val < best_loss:\n",
    "#             save_path = saver.save(sess, \"./simple_dnn_vege.ckpt\")\n",
    "#             best_loss = loss_val\n",
    "#             checks_without_progress = 0\n",
    "#         else:\n",
    "#             checks_without_progress += 1\n",
    "#             if checks_without_progress > max_checks_without_progress:\n",
    "#                 print(\"Early stopping!\")\n",
    "#                 break\n",
    "#         print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "#             epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"./simple_dnn_vege.ckpt\")\n",
    "#     acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "#     print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. error from previous cell:\n",
    "TypeError: only integer scalar arrays can be converted to a scalar index\n",
    "Solution:\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            for i in rnd_indices:\n",
    "                X_batch.append(X_train[i])\n",
    "                y_batch.append(y_train[i])\n",
    "\n",
    "- TEST ON vege_1_1 does your code in execution phase does the same as theirs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.placeholder(np.float32, shape=(None, n_channels), name=\"X\")\n",
    "        #here should go None, n_inputs for shape, but I have hardcoded it temporarily\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, np.float32), name=\"accuracy\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Make the important operations available easily through instance variables\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "    def fit(self, X, y, n_epochs=200, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Translate the labels vector to a vector of sorted class indices, containing\n",
    "        # integers from 0 to n_outputs - 1.\n",
    "        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n",
    "        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n",
    "        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # extra ops for batch normalization\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 50\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "#                 import pdb\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "#                     pdb.set_trace()\n",
    "#                     X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]\n",
    "                    X_batch = []\n",
    "                    y_batch = []\n",
    "                    for i in rnd_indices:\n",
    "                        X_batch.append(X_train[i])\n",
    "                        y_batch.append(y_train[i])\n",
    "                    X_batch = np.array(X_batch).astype(np.float32)\n",
    "#                     X_batch = tf.convert_to_tensor(X_batch, np.float32)\n",
    "                    y_batch = np.array(y_batch).astype(np.float32)\n",
    "#                     y_batch = tf.convert_to_tensor(y_batch, np.float32)\n",
    "#                     pdb.set_trace()\n",
    "#                     sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is used for importing randomizedsearchcv\n",
    "\n",
    "TRY ALSO USING GRID SEARCH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [20, 50, 100, 150, 200],\n",
    "    \"batch_size\": [500, 1000, 2000, 5000],\n",
    "    \"learning_rate\": [0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    \"batch_norm_momentum\": [0.9, 0.95, 0.98, 0.99],\n",
    "    \"dropout_rate\": [0.2, 0.4, 0.5, 0.6],\n",
    "    \"n_hidden_layers\": [2, 3, 4, 5, 8, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_neurons=150, n_hidden_layers=2, learning_rate=0.1, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.99, activation=<function elu at 0x7f93a9f21950> \n",
      "0\tValidation loss: 0.707068\tBest loss: 0.707068\tAccuracy: 76.12%\n",
      "1\tValidation loss: 0.705499\tBest loss: 0.705499\tAccuracy: 76.12%\n",
      "2\tValidation loss: 0.705151\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "3\tValidation loss: 0.714578\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "4\tValidation loss: 0.707459\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "5\tValidation loss: 0.708932\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "6\tValidation loss: 0.707173\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "7\tValidation loss: 0.710495\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "8\tValidation loss: 0.730948\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "9\tValidation loss: 0.726233\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "10\tValidation loss: 0.705576\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "11\tValidation loss: 0.713820\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "12\tValidation loss: 0.707560\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "13\tValidation loss: 0.713184\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "14\tValidation loss: 0.732383\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "15\tValidation loss: 0.711309\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "16\tValidation loss: 0.705712\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "17\tValidation loss: 0.708519\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "18\tValidation loss: 0.706554\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "19\tValidation loss: 0.707817\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "20\tValidation loss: 0.715062\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "21\tValidation loss: 0.716509\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "22\tValidation loss: 0.706263\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "23\tValidation loss: 0.718252\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "24\tValidation loss: 0.717329\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "25\tValidation loss: 0.771607\tBest loss: 0.705151\tAccuracy: 76.05%\n",
      "26\tValidation loss: 0.717512\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "27\tValidation loss: 0.706156\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "28\tValidation loss: 0.708775\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "29\tValidation loss: 0.756079\tBest loss: 0.705151\tAccuracy: 76.12%\n",
      "30\tValidation loss: 0.704432\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "31\tValidation loss: 0.712046\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "32\tValidation loss: 0.757088\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "33\tValidation loss: 0.723841\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "34\tValidation loss: 0.726549\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "35\tValidation loss: 0.719561\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "36\tValidation loss: 0.706038\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "37\tValidation loss: 0.744038\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "38\tValidation loss: 0.707117\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "39\tValidation loss: 0.710203\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "40\tValidation loss: 0.717894\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "41\tValidation loss: 0.711464\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "42\tValidation loss: 0.710806\tBest loss: 0.704432\tAccuracy: 76.12%\n",
      "43\tValidation loss: 0.704425\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "44\tValidation loss: 0.732021\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "45\tValidation loss: 0.768869\tBest loss: 0.704425\tAccuracy: 76.03%\n",
      "46\tValidation loss: 0.737729\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "47\tValidation loss: 0.721625\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "48\tValidation loss: 7.922699\tBest loss: 0.704425\tAccuracy: 9.65%\n",
      "49\tValidation loss: 0.707930\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "50\tValidation loss: 0.719079\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "51\tValidation loss: 0.705602\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "52\tValidation loss: 0.711972\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "53\tValidation loss: 0.713103\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "54\tValidation loss: 0.722756\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "55\tValidation loss: 0.725779\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "56\tValidation loss: 6.509852\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "57\tValidation loss: 0.716941\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "58\tValidation loss: 0.706753\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "59\tValidation loss: 0.712465\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "60\tValidation loss: 0.721251\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "61\tValidation loss: 0.709041\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "62\tValidation loss: 0.713816\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "63\tValidation loss: 0.734832\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "64\tValidation loss: 0.707397\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "65\tValidation loss: 0.731247\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "66\tValidation loss: 0.706918\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "67\tValidation loss: 0.710342\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "68\tValidation loss: 0.712269\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "69\tValidation loss: 0.715370\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "70\tValidation loss: 0.713261\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "71\tValidation loss: 0.706888\tBest loss: 0.704425\tAccuracy: 76.12%\n",
      "72\tValidation loss: 0.704060\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "73\tValidation loss: 0.709510\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "74\tValidation loss: 0.706695\tBest loss: 0.704060\tAccuracy: 76.11%\n",
      "75\tValidation loss: 0.798949\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "76\tValidation loss: 4.031667\tBest loss: 0.704060\tAccuracy: 76.10%\n",
      "77\tValidation loss: 0.705455\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "78\tValidation loss: 0.718990\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "79\tValidation loss: 0.707920\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "80\tValidation loss: 0.707291\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "81\tValidation loss: 0.710288\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "82\tValidation loss: 0.772911\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "83\tValidation loss: 0.708351\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "84\tValidation loss: 0.753425\tBest loss: 0.704060\tAccuracy: 76.08%\n",
      "85\tValidation loss: 0.707010\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "86\tValidation loss: 0.711478\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "87\tValidation loss: 0.704943\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "88\tValidation loss: 0.714101\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "89\tValidation loss: 0.722314\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "90\tValidation loss: 0.708988\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "91\tValidation loss: 0.716977\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "92\tValidation loss: 0.708858\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "93\tValidation loss: 1.083199\tBest loss: 0.704060\tAccuracy: 76.09%\n",
      "94\tValidation loss: 0.707954\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "95\tValidation loss: 0.704802\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "96\tValidation loss: 0.728277\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "97\tValidation loss: 0.704484\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "98\tValidation loss: 0.705856\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "99\tValidation loss: 0.705768\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "100\tValidation loss: 0.840827\tBest loss: 0.704060\tAccuracy: 72.90%\n",
      "101\tValidation loss: 0.717062\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "102\tValidation loss: 0.709516\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "103\tValidation loss: 0.717339\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "104\tValidation loss: 0.726525\tBest loss: 0.704060\tAccuracy: 76.09%\n",
      "105\tValidation loss: 0.713818\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "106\tValidation loss: 0.711592\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "107\tValidation loss: 0.807994\tBest loss: 0.704060\tAccuracy: 76.08%\n",
      "108\tValidation loss: 0.707582\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "109\tValidation loss: 0.709025\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "110\tValidation loss: 0.707155\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "111\tValidation loss: 0.725542\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "112\tValidation loss: 0.744107\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "113\tValidation loss: 0.713644\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "114\tValidation loss: 0.734926\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "115\tValidation loss: 0.708709\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "116\tValidation loss: 0.727051\tBest loss: 0.704060\tAccuracy: 76.06%\n",
      "117\tValidation loss: 0.708683\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "118\tValidation loss: 0.731295\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "119\tValidation loss: 0.704461\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "120\tValidation loss: 1.723215\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "121\tValidation loss: 0.709692\tBest loss: 0.704060\tAccuracy: 76.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\tValidation loss: 0.720272\tBest loss: 0.704060\tAccuracy: 76.12%\n",
      "123\tValidation loss: 1.229696\tBest loss: 0.704060\tAccuracy: 76.11%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=150, n_hidden_layers=2, learning_rate=0.1, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.99, activation=<function elu at 0x7f93a9f21950>, total=17.9min\n",
      "[CV] n_neurons=150, n_hidden_layers=2, learning_rate=0.1, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.99, activation=<function elu at 0x7f93a9f21950> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 18.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.707069\tBest loss: 0.707069\tAccuracy: 76.12%\n",
      "1\tValidation loss: 0.705713\tBest loss: 0.705713\tAccuracy: 76.12%\n",
      "2\tValidation loss: 0.704268\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "3\tValidation loss: 0.713686\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "4\tValidation loss: 0.710482\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "5\tValidation loss: 0.709039\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "6\tValidation loss: 0.706847\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "7\tValidation loss: 0.720610\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "8\tValidation loss: 0.704820\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "9\tValidation loss: 0.708895\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "10\tValidation loss: 0.708158\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "11\tValidation loss: 0.707452\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "12\tValidation loss: 0.841007\tBest loss: 0.704268\tAccuracy: 76.11%\n",
      "13\tValidation loss: 0.709945\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "14\tValidation loss: 0.719240\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "15\tValidation loss: 0.713470\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "16\tValidation loss: 0.705147\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "17\tValidation loss: 0.705256\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "18\tValidation loss: 0.710983\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "19\tValidation loss: 0.710144\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "20\tValidation loss: 0.708921\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "21\tValidation loss: 0.707014\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "22\tValidation loss: 0.712513\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "23\tValidation loss: 0.709252\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "24\tValidation loss: 0.710947\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "25\tValidation loss: 0.709371\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "26\tValidation loss: 0.709176\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "27\tValidation loss: 0.706898\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "28\tValidation loss: 0.707365\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "29\tValidation loss: 0.714171\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "30\tValidation loss: 0.717783\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "31\tValidation loss: 0.711083\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "32\tValidation loss: 0.708450\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "33\tValidation loss: 0.725495\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "34\tValidation loss: 0.724338\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "35\tValidation loss: 0.708555\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "36\tValidation loss: 0.710081\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "37\tValidation loss: 0.747127\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "38\tValidation loss: 0.706566\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "39\tValidation loss: 0.705814\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "40\tValidation loss: 0.707674\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "41\tValidation loss: 0.717708\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "42\tValidation loss: 0.707359\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "43\tValidation loss: 0.714269\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "44\tValidation loss: 0.721005\tBest loss: 0.704268\tAccuracy: 76.09%\n",
      "45\tValidation loss: 0.721164\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "46\tValidation loss: 0.706758\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "47\tValidation loss: 0.706827\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "48\tValidation loss: 0.711593\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "49\tValidation loss: 0.712103\tBest loss: 0.704268\tAccuracy: 76.09%\n",
      "50\tValidation loss: 0.714427\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "51\tValidation loss: 0.707676\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "52\tValidation loss: 0.716057\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "53\tValidation loss: 0.707631\tBest loss: 0.704268\tAccuracy: 76.12%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=150, n_hidden_layers=2, learning_rate=0.1, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.99, activation=<function elu at 0x7f93a9f21950>, total= 8.1min\n",
      "[CV] n_neurons=150, n_hidden_layers=2, learning_rate=0.1, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.99, activation=<function elu at 0x7f93a9f21950> \n",
      "0\tValidation loss: 0.707068\tBest loss: 0.707068\tAccuracy: 76.12%\n",
      "1\tValidation loss: 0.705862\tBest loss: 0.705862\tAccuracy: 76.12%\n",
      "2\tValidation loss: 0.704707\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "3\tValidation loss: 0.712421\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "4\tValidation loss: 0.707639\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "5\tValidation loss: 0.706164\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "6\tValidation loss: 0.705263\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "7\tValidation loss: 0.719815\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "8\tValidation loss: 0.706510\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "9\tValidation loss: 0.704771\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "10\tValidation loss: 0.714707\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "11\tValidation loss: 0.708407\tBest loss: 0.704707\tAccuracy: 76.12%\n",
      "12\tValidation loss: 0.704489\tBest loss: 0.704489\tAccuracy: 76.12%\n",
      "13\tValidation loss: 0.708951\tBest loss: 0.704489\tAccuracy: 76.09%\n",
      "14\tValidation loss: 0.704714\tBest loss: 0.704489\tAccuracy: 76.12%\n",
      "15\tValidation loss: 0.705499\tBest loss: 0.704489\tAccuracy: 76.12%\n",
      "16\tValidation loss: 0.716912\tBest loss: 0.704489\tAccuracy: 76.12%\n",
      "17\tValidation loss: 0.710971\tBest loss: 0.704489\tAccuracy: 76.12%\n",
      "18\tValidation loss: 0.704233\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "19\tValidation loss: 0.707457\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "20\tValidation loss: 0.704575\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "21\tValidation loss: 0.710299\tBest loss: 0.704233\tAccuracy: 76.07%\n",
      "22\tValidation loss: 0.710029\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "23\tValidation loss: 7.667725\tBest loss: 0.704233\tAccuracy: 75.06%\n",
      "24\tValidation loss: 0.709299\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "25\tValidation loss: 0.712285\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "26\tValidation loss: 0.740337\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "27\tValidation loss: 0.706594\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "28\tValidation loss: 0.717118\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "29\tValidation loss: 0.705149\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "30\tValidation loss: 0.705529\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "31\tValidation loss: 0.750107\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "32\tValidation loss: 0.706315\tBest loss: 0.704233\tAccuracy: 76.09%\n",
      "33\tValidation loss: 0.709070\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "34\tValidation loss: 0.709591\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "35\tValidation loss: 0.708503\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "36\tValidation loss: 0.715147\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "37\tValidation loss: 0.721256\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "38\tValidation loss: 0.706193\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "39\tValidation loss: 0.709566\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "40\tValidation loss: 0.707680\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "41\tValidation loss: 0.704273\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "42\tValidation loss: 0.713342\tBest loss: 0.704233\tAccuracy: 76.07%\n",
      "43\tValidation loss: 0.706692\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "44\tValidation loss: 0.726226\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "45\tValidation loss: 0.706307\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "46\tValidation loss: 0.712527\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "47\tValidation loss: 0.714599\tBest loss: 0.704233\tAccuracy: 76.10%\n",
      "48\tValidation loss: 0.718223\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "49\tValidation loss: 0.704353\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "50\tValidation loss: 0.704748\tBest loss: 0.704233\tAccuracy: 76.11%\n",
      "51\tValidation loss: 0.706469\tBest loss: 0.704233\tAccuracy: 76.10%\n",
      "52\tValidation loss: 0.710295\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "53\tValidation loss: 0.705008\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "54\tValidation loss: 0.746731\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "55\tValidation loss: 0.718423\tBest loss: 0.704233\tAccuracy: 76.08%\n",
      "56\tValidation loss: 0.711315\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "57\tValidation loss: 0.706760\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "58\tValidation loss: 0.718972\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "59\tValidation loss: 0.708196\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "60\tValidation loss: 0.709660\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "61\tValidation loss: 0.706737\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "62\tValidation loss: 0.707243\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "63\tValidation loss: 0.712089\tBest loss: 0.704233\tAccuracy: 76.11%\n",
      "64\tValidation loss: 0.705036\tBest loss: 0.704233\tAccuracy: 76.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\tValidation loss: 0.713176\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "66\tValidation loss: 0.707524\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "67\tValidation loss: 0.705679\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "68\tValidation loss: 0.709862\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "69\tValidation loss: 0.713713\tBest loss: 0.704233\tAccuracy: 76.12%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=150, n_hidden_layers=2, learning_rate=0.1, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.99, activation=<function elu at 0x7f93a9f21950>, total=10.4min\n",
      "[CV] n_neurons=50, n_hidden_layers=2, learning_rate=0.05, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function relu at 0x7f93a9f4ec80> \n",
      "0\tValidation loss: 0.705316\tBest loss: 0.705316\tAccuracy: 76.12%\n",
      "1\tValidation loss: 0.704934\tBest loss: 0.704934\tAccuracy: 76.12%\n",
      "2\tValidation loss: 0.704399\tBest loss: 0.704399\tAccuracy: 76.12%\n",
      "3\tValidation loss: 0.705243\tBest loss: 0.704399\tAccuracy: 76.12%\n",
      "4\tValidation loss: 0.705281\tBest loss: 0.704399\tAccuracy: 76.12%\n",
      "5\tValidation loss: 0.704720\tBest loss: 0.704399\tAccuracy: 76.12%\n",
      "6\tValidation loss: 0.705418\tBest loss: 0.704399\tAccuracy: 76.12%\n",
      "7\tValidation loss: 0.704523\tBest loss: 0.704399\tAccuracy: 76.12%\n",
      "8\tValidation loss: 0.704349\tBest loss: 0.704349\tAccuracy: 76.12%\n",
      "9\tValidation loss: 0.704213\tBest loss: 0.704213\tAccuracy: 76.12%\n",
      "10\tValidation loss: 0.704481\tBest loss: 0.704213\tAccuracy: 76.12%\n",
      "11\tValidation loss: 0.704002\tBest loss: 0.704002\tAccuracy: 76.12%\n",
      "12\tValidation loss: 0.706930\tBest loss: 0.704002\tAccuracy: 76.12%\n",
      "13\tValidation loss: 0.704495\tBest loss: 0.704002\tAccuracy: 76.12%\n",
      "14\tValidation loss: 0.703903\tBest loss: 0.703903\tAccuracy: 76.12%\n",
      "15\tValidation loss: 0.703928\tBest loss: 0.703903\tAccuracy: 76.12%\n",
      "16\tValidation loss: 0.713027\tBest loss: 0.703903\tAccuracy: 75.86%\n",
      "17\tValidation loss: 0.705437\tBest loss: 0.703903\tAccuracy: 76.12%\n",
      "18\tValidation loss: 0.704963\tBest loss: 0.703903\tAccuracy: 76.12%\n",
      "19\tValidation loss: 0.703817\tBest loss: 0.703817\tAccuracy: 76.12%\n",
      "20\tValidation loss: 0.705027\tBest loss: 0.703817\tAccuracy: 76.12%\n",
      "21\tValidation loss: 0.704696\tBest loss: 0.703817\tAccuracy: 76.12%\n",
      "22\tValidation loss: 0.704493\tBest loss: 0.703817\tAccuracy: 76.12%\n",
      "23\tValidation loss: 0.704621\tBest loss: 0.703817\tAccuracy: 76.12%\n",
      "24\tValidation loss: 0.705464\tBest loss: 0.703817\tAccuracy: 76.12%\n",
      "25\tValidation loss: 0.703175\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "26\tValidation loss: 0.704000\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "27\tValidation loss: 0.704891\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "28\tValidation loss: 0.703992\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "29\tValidation loss: 0.704216\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "30\tValidation loss: 0.703277\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "31\tValidation loss: 0.704947\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "32\tValidation loss: 0.703631\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "33\tValidation loss: 0.704672\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "34\tValidation loss: 0.704464\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "35\tValidation loss: 0.703744\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "36\tValidation loss: 0.705248\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "37\tValidation loss: 0.704328\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "38\tValidation loss: 0.704185\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "39\tValidation loss: 0.703447\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "40\tValidation loss: 0.704003\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "41\tValidation loss: 0.704264\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "42\tValidation loss: 0.703877\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "43\tValidation loss: 0.704470\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "44\tValidation loss: 0.704690\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "45\tValidation loss: 0.704450\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "46\tValidation loss: 0.704364\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "47\tValidation loss: 0.706062\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "48\tValidation loss: 0.704655\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "49\tValidation loss: 0.704837\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "50\tValidation loss: 0.704964\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "51\tValidation loss: 0.705472\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "52\tValidation loss: 0.705021\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "53\tValidation loss: 0.705799\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "54\tValidation loss: 0.704065\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "55\tValidation loss: 0.705037\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "56\tValidation loss: 0.704325\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "57\tValidation loss: 0.704467\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "58\tValidation loss: 0.704052\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "59\tValidation loss: 0.704747\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "60\tValidation loss: 0.703812\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "61\tValidation loss: 0.707749\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "62\tValidation loss: 0.703981\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "63\tValidation loss: 0.704223\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "64\tValidation loss: 0.705240\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "65\tValidation loss: 0.704570\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "66\tValidation loss: 0.704515\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "67\tValidation loss: 0.703970\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "68\tValidation loss: 0.705646\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "69\tValidation loss: 0.704266\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "70\tValidation loss: 0.704775\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "71\tValidation loss: 0.704562\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "72\tValidation loss: 0.703566\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "73\tValidation loss: 0.704905\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "74\tValidation loss: 0.704392\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "75\tValidation loss: 0.704503\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "76\tValidation loss: 0.704579\tBest loss: 0.703175\tAccuracy: 76.12%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=50, n_hidden_layers=2, learning_rate=0.05, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function relu at 0x7f93a9f4ec80>, total=11.5min\n",
      "[CV] n_neurons=50, n_hidden_layers=2, learning_rate=0.05, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function relu at 0x7f93a9f4ec80> \n",
      "0\tValidation loss: 0.705471\tBest loss: 0.705471\tAccuracy: 76.12%\n",
      "1\tValidation loss: 0.704610\tBest loss: 0.704610\tAccuracy: 76.12%\n",
      "2\tValidation loss: 0.704474\tBest loss: 0.704474\tAccuracy: 76.12%\n",
      "3\tValidation loss: 0.704732\tBest loss: 0.704474\tAccuracy: 76.12%\n",
      "4\tValidation loss: 0.704277\tBest loss: 0.704277\tAccuracy: 76.12%\n",
      "5\tValidation loss: 0.705908\tBest loss: 0.704277\tAccuracy: 76.12%\n",
      "6\tValidation loss: 0.704546\tBest loss: 0.704277\tAccuracy: 76.12%\n",
      "7\tValidation loss: 0.704860\tBest loss: 0.704277\tAccuracy: 76.12%\n",
      "8\tValidation loss: 0.704106\tBest loss: 0.704106\tAccuracy: 76.12%\n",
      "9\tValidation loss: 0.703943\tBest loss: 0.703943\tAccuracy: 76.12%\n",
      "10\tValidation loss: 0.704847\tBest loss: 0.703943\tAccuracy: 76.12%\n",
      "11\tValidation loss: 0.703909\tBest loss: 0.703909\tAccuracy: 76.12%\n",
      "12\tValidation loss: 0.706442\tBest loss: 0.703909\tAccuracy: 76.12%\n",
      "13\tValidation loss: 0.703849\tBest loss: 0.703849\tAccuracy: 76.12%\n",
      "14\tValidation loss: 0.703620\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "15\tValidation loss: 0.705068\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "16\tValidation loss: 0.704109\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "17\tValidation loss: 0.705236\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "18\tValidation loss: 0.704839\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "19\tValidation loss: 0.704073\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "20\tValidation loss: 0.705481\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "21\tValidation loss: 0.704884\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "22\tValidation loss: 0.704796\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "23\tValidation loss: 0.704480\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "24\tValidation loss: 0.705203\tBest loss: 0.703620\tAccuracy: 76.12%\n",
      "25\tValidation loss: 0.703189\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "26\tValidation loss: 0.703691\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "27\tValidation loss: 0.704504\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "28\tValidation loss: 0.704739\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "29\tValidation loss: 0.704419\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "30\tValidation loss: 0.703393\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "31\tValidation loss: 0.704504\tBest loss: 0.703189\tAccuracy: 76.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\tValidation loss: 0.703515\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "33\tValidation loss: 0.705410\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "34\tValidation loss: 0.704640\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "35\tValidation loss: 0.703867\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "36\tValidation loss: 0.705479\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "37\tValidation loss: 0.704491\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "38\tValidation loss: 0.703744\tBest loss: 0.703189\tAccuracy: 76.12%\n",
      "39\tValidation loss: 0.703061\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "40\tValidation loss: 0.703844\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "41\tValidation loss: 0.704648\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "42\tValidation loss: 0.704093\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "43\tValidation loss: 0.704125\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "44\tValidation loss: 0.704175\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "45\tValidation loss: 0.704745\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "46\tValidation loss: 0.703663\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "47\tValidation loss: 0.706022\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "48\tValidation loss: 0.704299\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "49\tValidation loss: 0.704448\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "50\tValidation loss: 0.705144\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "51\tValidation loss: 0.705218\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "52\tValidation loss: 0.704658\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "53\tValidation loss: 0.705845\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "54\tValidation loss: 0.704163\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "55\tValidation loss: 0.705698\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "56\tValidation loss: 0.705199\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "57\tValidation loss: 0.704860\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "58\tValidation loss: 0.704112\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "59\tValidation loss: 0.704731\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "60\tValidation loss: 0.703573\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "61\tValidation loss: 0.707389\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "62\tValidation loss: 0.703929\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "63\tValidation loss: 0.703478\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "64\tValidation loss: 0.705466\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "65\tValidation loss: 0.704329\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "66\tValidation loss: 0.704414\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "67\tValidation loss: 0.703832\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "68\tValidation loss: 0.705424\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "69\tValidation loss: 0.704689\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "70\tValidation loss: 0.705355\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "71\tValidation loss: 0.703969\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "72\tValidation loss: 0.703358\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "73\tValidation loss: 0.704386\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "74\tValidation loss: 0.704017\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "75\tValidation loss: 0.704589\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "76\tValidation loss: 0.703743\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "77\tValidation loss: 0.703600\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "78\tValidation loss: 0.703305\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "79\tValidation loss: 0.704025\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "80\tValidation loss: 0.704636\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "81\tValidation loss: 0.705236\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "82\tValidation loss: 0.704131\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "83\tValidation loss: 0.704839\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "84\tValidation loss: 0.707911\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "85\tValidation loss: 0.703916\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "86\tValidation loss: 0.703285\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "87\tValidation loss: 0.703740\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "88\tValidation loss: 0.703099\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "89\tValidation loss: 0.705358\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "90\tValidation loss: 0.704144\tBest loss: 0.703061\tAccuracy: 76.12%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=50, n_hidden_layers=2, learning_rate=0.05, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function relu at 0x7f93a9f4ec80>, total=13.5min\n",
      "[CV] n_neurons=50, n_hidden_layers=2, learning_rate=0.05, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function relu at 0x7f93a9f4ec80> \n",
      "0\tValidation loss: 0.705163\tBest loss: 0.705163\tAccuracy: 76.12%\n",
      "1\tValidation loss: 0.704616\tBest loss: 0.704616\tAccuracy: 76.12%\n",
      "2\tValidation loss: 0.704359\tBest loss: 0.704359\tAccuracy: 76.12%\n",
      "3\tValidation loss: 0.705726\tBest loss: 0.704359\tAccuracy: 76.12%\n",
      "4\tValidation loss: 0.704226\tBest loss: 0.704226\tAccuracy: 76.12%\n",
      "5\tValidation loss: 0.705395\tBest loss: 0.704226\tAccuracy: 76.12%\n",
      "6\tValidation loss: 0.704614\tBest loss: 0.704226\tAccuracy: 76.12%\n",
      "7\tValidation loss: 0.704461\tBest loss: 0.704226\tAccuracy: 76.12%\n",
      "8\tValidation loss: 0.704291\tBest loss: 0.704226\tAccuracy: 76.12%\n",
      "9\tValidation loss: 0.704165\tBest loss: 0.704165\tAccuracy: 76.12%\n",
      "10\tValidation loss: 0.705564\tBest loss: 0.704165\tAccuracy: 76.12%\n",
      "11\tValidation loss: 0.704056\tBest loss: 0.704056\tAccuracy: 76.12%\n",
      "12\tValidation loss: 0.706145\tBest loss: 0.704056\tAccuracy: 76.12%\n",
      "13\tValidation loss: 0.704082\tBest loss: 0.704056\tAccuracy: 76.12%\n",
      "14\tValidation loss: 0.703699\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "15\tValidation loss: 0.705177\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "16\tValidation loss: 0.704405\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "17\tValidation loss: 0.705953\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "18\tValidation loss: 0.704362\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "19\tValidation loss: 0.704710\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "20\tValidation loss: 0.704783\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "21\tValidation loss: 0.706238\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "22\tValidation loss: 0.704674\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "23\tValidation loss: 0.704074\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "24\tValidation loss: 0.704830\tBest loss: 0.703699\tAccuracy: 76.12%\n",
      "25\tValidation loss: 0.703372\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "26\tValidation loss: 0.703674\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "27\tValidation loss: 0.703834\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "28\tValidation loss: 0.704194\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "29\tValidation loss: 0.704268\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "30\tValidation loss: 0.703722\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "31\tValidation loss: 0.705295\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "32\tValidation loss: 0.703375\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "33\tValidation loss: 0.705114\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "34\tValidation loss: 0.704624\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "35\tValidation loss: 0.703874\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "36\tValidation loss: 0.705005\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "37\tValidation loss: 0.703807\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "38\tValidation loss: 0.703994\tBest loss: 0.703372\tAccuracy: 76.12%\n",
      "39\tValidation loss: 0.703135\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "40\tValidation loss: 0.703841\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "41\tValidation loss: 0.704777\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "42\tValidation loss: 0.704268\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "43\tValidation loss: 0.704310\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "44\tValidation loss: 0.704665\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "45\tValidation loss: 0.704187\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "46\tValidation loss: 0.703801\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "47\tValidation loss: 0.706526\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "48\tValidation loss: 0.705391\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "49\tValidation loss: 0.704502\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "50\tValidation loss: 0.704621\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "51\tValidation loss: 0.705842\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "52\tValidation loss: 0.704274\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "53\tValidation loss: 0.706062\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "54\tValidation loss: 0.703914\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "55\tValidation loss: 0.705360\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "56\tValidation loss: 0.704377\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "57\tValidation loss: 0.704578\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "58\tValidation loss: 0.704198\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "59\tValidation loss: 0.704345\tBest loss: 0.703135\tAccuracy: 76.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\tValidation loss: 0.703509\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "61\tValidation loss: 0.706996\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "62\tValidation loss: 0.703331\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "63\tValidation loss: 0.703376\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "64\tValidation loss: 0.705414\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "65\tValidation loss: 0.703862\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "66\tValidation loss: 0.704443\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "67\tValidation loss: 0.704066\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "68\tValidation loss: 0.706330\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "69\tValidation loss: 0.704251\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "70\tValidation loss: 0.704971\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "71\tValidation loss: 0.704296\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "72\tValidation loss: 0.703212\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "73\tValidation loss: 0.704980\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "74\tValidation loss: 0.704249\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "75\tValidation loss: 0.704500\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "76\tValidation loss: 0.703871\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "77\tValidation loss: 0.703805\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "78\tValidation loss: 0.703427\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "79\tValidation loss: 0.704003\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "80\tValidation loss: 0.704551\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "81\tValidation loss: 0.705626\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "82\tValidation loss: 0.703939\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "83\tValidation loss: 0.705233\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "84\tValidation loss: 0.707344\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "85\tValidation loss: 0.704026\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "86\tValidation loss: 0.703793\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "87\tValidation loss: 0.703869\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "88\tValidation loss: 0.703237\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "89\tValidation loss: 0.704727\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "90\tValidation loss: 0.704278\tBest loss: 0.703135\tAccuracy: 76.12%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=50, n_hidden_layers=2, learning_rate=0.05, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function relu at 0x7f93a9f4ec80>, total=13.5min\n",
      "[CV] n_neurons=100, n_hidden_layers=8, learning_rate=0.02, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f921a9c57b8> \n",
      "0\tValidation loss: 0.705615\tBest loss: 0.705615\tAccuracy: 76.12%\n",
      "1\tValidation loss: 0.703814\tBest loss: 0.703814\tAccuracy: 76.12%\n",
      "2\tValidation loss: 0.703851\tBest loss: 0.703814\tAccuracy: 76.12%\n",
      "3\tValidation loss: 0.704678\tBest loss: 0.703814\tAccuracy: 76.12%\n",
      "4\tValidation loss: 0.703328\tBest loss: 0.703328\tAccuracy: 76.12%\n",
      "5\tValidation loss: 0.703085\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "6\tValidation loss: 0.704553\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "7\tValidation loss: 0.704439\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "8\tValidation loss: 0.705252\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "9\tValidation loss: 0.704534\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "10\tValidation loss: 0.703791\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "11\tValidation loss: 0.704824\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "12\tValidation loss: 0.703713\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "13\tValidation loss: 0.704034\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "14\tValidation loss: 0.703458\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "15\tValidation loss: 0.704607\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "16\tValidation loss: 0.704585\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "17\tValidation loss: 0.705071\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "18\tValidation loss: 0.703650\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "19\tValidation loss: 0.705128\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "20\tValidation loss: 0.704159\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "21\tValidation loss: 0.704192\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "22\tValidation loss: 0.703823\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "23\tValidation loss: 0.704351\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "24\tValidation loss: 0.703655\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "25\tValidation loss: 0.703856\tBest loss: 0.703085\tAccuracy: 76.12%\n",
      "26\tValidation loss: 0.702863\tBest loss: 0.702863\tAccuracy: 76.13%\n",
      "27\tValidation loss: 0.703345\tBest loss: 0.702863\tAccuracy: 76.13%\n",
      "28\tValidation loss: 0.703465\tBest loss: 0.702863\tAccuracy: 76.12%\n",
      "29\tValidation loss: 0.703718\tBest loss: 0.702863\tAccuracy: 76.13%\n",
      "30\tValidation loss: 0.704096\tBest loss: 0.702863\tAccuracy: 76.13%\n",
      "31\tValidation loss: 0.703528\tBest loss: 0.702863\tAccuracy: 76.13%\n",
      "32\tValidation loss: 0.702764\tBest loss: 0.702764\tAccuracy: 76.13%\n",
      "33\tValidation loss: 0.702363\tBest loss: 0.702363\tAccuracy: 76.13%\n",
      "34\tValidation loss: 0.703954\tBest loss: 0.702363\tAccuracy: 76.13%\n",
      "35\tValidation loss: 0.702361\tBest loss: 0.702361\tAccuracy: 76.13%\n",
      "36\tValidation loss: 0.703277\tBest loss: 0.702361\tAccuracy: 76.13%\n",
      "37\tValidation loss: 0.704915\tBest loss: 0.702361\tAccuracy: 76.13%\n",
      "38\tValidation loss: 0.703573\tBest loss: 0.702361\tAccuracy: 76.13%\n",
      "39\tValidation loss: 0.703148\tBest loss: 0.702361\tAccuracy: 76.13%\n",
      "40\tValidation loss: 0.702756\tBest loss: 0.702361\tAccuracy: 76.13%\n",
      "41\tValidation loss: 0.703626\tBest loss: 0.702361\tAccuracy: 76.12%\n",
      "42\tValidation loss: 0.701595\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "43\tValidation loss: 0.703335\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "44\tValidation loss: 0.704569\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "45\tValidation loss: 0.703615\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "46\tValidation loss: 0.702886\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "47\tValidation loss: 0.702729\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "48\tValidation loss: 0.702796\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "49\tValidation loss: 0.703122\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "50\tValidation loss: 0.704146\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "51\tValidation loss: 0.702726\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "52\tValidation loss: 0.703861\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "53\tValidation loss: 0.703046\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "54\tValidation loss: 0.701784\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "55\tValidation loss: 0.704016\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "56\tValidation loss: 0.703163\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "57\tValidation loss: 0.701898\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "58\tValidation loss: 0.703112\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "59\tValidation loss: 0.702840\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "60\tValidation loss: 0.703308\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "61\tValidation loss: 0.702868\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "62\tValidation loss: 0.702204\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "63\tValidation loss: 0.703180\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "64\tValidation loss: 0.702974\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "65\tValidation loss: 0.703791\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "66\tValidation loss: 0.703096\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "67\tValidation loss: 0.702732\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "68\tValidation loss: 0.702125\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "69\tValidation loss: 0.703157\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "70\tValidation loss: 0.704130\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "71\tValidation loss: 0.702878\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "72\tValidation loss: 0.702755\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "73\tValidation loss: 0.702605\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "74\tValidation loss: 0.703484\tBest loss: 0.701595\tAccuracy: 76.11%\n",
      "75\tValidation loss: 0.702886\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "76\tValidation loss: 0.702506\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "77\tValidation loss: 0.702073\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "78\tValidation loss: 0.702275\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "79\tValidation loss: 0.702429\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "80\tValidation loss: 0.703114\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "81\tValidation loss: 0.703203\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "82\tValidation loss: 0.703355\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "83\tValidation loss: 0.703159\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "84\tValidation loss: 0.704263\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "85\tValidation loss: 0.702297\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "86\tValidation loss: 0.702030\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "87\tValidation loss: 0.703004\tBest loss: 0.701595\tAccuracy: 76.13%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\tValidation loss: 0.703053\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "89\tValidation loss: 0.703347\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "90\tValidation loss: 0.702817\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "91\tValidation loss: 0.702325\tBest loss: 0.701595\tAccuracy: 76.13%\n",
      "92\tValidation loss: 0.702247\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "93\tValidation loss: 0.702783\tBest loss: 0.701595\tAccuracy: 76.14%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, n_hidden_layers=8, learning_rate=0.02, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f921a9c57b8>, total=31.2min\n",
      "[CV] n_neurons=100, n_hidden_layers=8, learning_rate=0.02, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f921a9c57b8> \n",
      "0\tValidation loss: 0.705473\tBest loss: 0.705473\tAccuracy: 76.12%\n",
      "1\tValidation loss: 0.703205\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "2\tValidation loss: 0.703272\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "3\tValidation loss: 0.706511\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "4\tValidation loss: 0.703344\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "5\tValidation loss: 0.703462\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "6\tValidation loss: 0.704620\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "7\tValidation loss: 0.703914\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "8\tValidation loss: 0.704444\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "9\tValidation loss: 0.704540\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "10\tValidation loss: 0.703560\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "11\tValidation loss: 0.704366\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "12\tValidation loss: 0.704429\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "13\tValidation loss: 0.704237\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "14\tValidation loss: 0.703579\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "15\tValidation loss: 0.704544\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "16\tValidation loss: 0.705242\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "17\tValidation loss: 0.704857\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "18\tValidation loss: 0.703922\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "19\tValidation loss: 0.705121\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "20\tValidation loss: 0.704723\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "21\tValidation loss: 0.704637\tBest loss: 0.703205\tAccuracy: 76.12%\n",
      "22\tValidation loss: 0.702933\tBest loss: 0.702933\tAccuracy: 76.12%\n",
      "23\tValidation loss: 0.705208\tBest loss: 0.702933\tAccuracy: 76.13%\n",
      "24\tValidation loss: 0.704188\tBest loss: 0.702933\tAccuracy: 76.12%\n",
      "25\tValidation loss: 0.704412\tBest loss: 0.702933\tAccuracy: 76.12%\n",
      "26\tValidation loss: 0.702793\tBest loss: 0.702793\tAccuracy: 76.12%\n",
      "27\tValidation loss: 0.702868\tBest loss: 0.702793\tAccuracy: 76.12%\n",
      "28\tValidation loss: 0.703741\tBest loss: 0.702793\tAccuracy: 76.12%\n",
      "29\tValidation loss: 0.703697\tBest loss: 0.702793\tAccuracy: 76.13%\n",
      "30\tValidation loss: 0.703489\tBest loss: 0.702793\tAccuracy: 76.12%\n",
      "31\tValidation loss: 0.703170\tBest loss: 0.702793\tAccuracy: 76.12%\n",
      "32\tValidation loss: 0.703700\tBest loss: 0.702793\tAccuracy: 76.13%\n",
      "33\tValidation loss: 0.703009\tBest loss: 0.702793\tAccuracy: 76.13%\n",
      "34\tValidation loss: 0.703488\tBest loss: 0.702793\tAccuracy: 76.13%\n",
      "35\tValidation loss: 0.703071\tBest loss: 0.702793\tAccuracy: 76.13%\n",
      "36\tValidation loss: 0.703962\tBest loss: 0.702793\tAccuracy: 76.13%\n",
      "37\tValidation loss: 0.705474\tBest loss: 0.702793\tAccuracy: 76.13%\n",
      "38\tValidation loss: 0.703692\tBest loss: 0.702793\tAccuracy: 76.13%\n",
      "39\tValidation loss: 0.703410\tBest loss: 0.702793\tAccuracy: 76.13%\n",
      "40\tValidation loss: 0.702368\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "41\tValidation loss: 0.704473\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "42\tValidation loss: 0.702396\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "43\tValidation loss: 0.703115\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "44\tValidation loss: 0.703540\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "45\tValidation loss: 0.703607\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "46\tValidation loss: 0.702760\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "47\tValidation loss: 0.703063\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "48\tValidation loss: 0.703108\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "49\tValidation loss: 0.703348\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "50\tValidation loss: 0.703992\tBest loss: 0.702368\tAccuracy: 76.13%\n",
      "51\tValidation loss: 0.702279\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "52\tValidation loss: 0.703916\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "53\tValidation loss: 0.703165\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "54\tValidation loss: 0.702772\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "55\tValidation loss: 0.703077\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "56\tValidation loss: 0.703788\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "57\tValidation loss: 0.702558\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "58\tValidation loss: 0.703313\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "59\tValidation loss: 0.703627\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "60\tValidation loss: 0.703381\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "61\tValidation loss: 0.702940\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "62\tValidation loss: 0.702902\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "63\tValidation loss: 0.703156\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "64\tValidation loss: 0.703113\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "65\tValidation loss: 0.704090\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "66\tValidation loss: 0.703143\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "67\tValidation loss: 0.703549\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "68\tValidation loss: 0.703177\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "69\tValidation loss: 0.703691\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "70\tValidation loss: 0.703728\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "71\tValidation loss: 0.703897\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "72\tValidation loss: 0.702915\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "73\tValidation loss: 0.702961\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "74\tValidation loss: 0.703425\tBest loss: 0.702279\tAccuracy: 76.14%\n",
      "75\tValidation loss: 0.703134\tBest loss: 0.702279\tAccuracy: 76.14%\n",
      "76\tValidation loss: 0.703047\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "77\tValidation loss: 0.703114\tBest loss: 0.702279\tAccuracy: 76.13%\n",
      "78\tValidation loss: 0.702247\tBest loss: 0.702247\tAccuracy: 76.13%\n",
      "79\tValidation loss: 0.702140\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "80\tValidation loss: 0.703719\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "81\tValidation loss: 0.703234\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "82\tValidation loss: 0.702922\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "83\tValidation loss: 0.704280\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "84\tValidation loss: 0.705194\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "85\tValidation loss: 0.702620\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "86\tValidation loss: 0.702793\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "87\tValidation loss: 0.702945\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "88\tValidation loss: 0.703350\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "89\tValidation loss: 0.703806\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "90\tValidation loss: 0.702961\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "91\tValidation loss: 0.702809\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "92\tValidation loss: 0.702337\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "93\tValidation loss: 0.702599\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "94\tValidation loss: 0.703573\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "95\tValidation loss: 0.703130\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "96\tValidation loss: 0.702614\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "97\tValidation loss: 0.704632\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "98\tValidation loss: 0.703665\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "99\tValidation loss: 0.704353\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "100\tValidation loss: 0.703073\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "101\tValidation loss: 0.704024\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "102\tValidation loss: 0.703381\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "103\tValidation loss: 0.703679\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "104\tValidation loss: 0.702644\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "105\tValidation loss: 0.704029\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "106\tValidation loss: 0.702471\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "107\tValidation loss: 0.702788\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "108\tValidation loss: 0.704287\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "109\tValidation loss: 0.702960\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "110\tValidation loss: 0.702546\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "111\tValidation loss: 0.703949\tBest loss: 0.702140\tAccuracy: 76.14%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\tValidation loss: 0.704132\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "113\tValidation loss: 0.703458\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "114\tValidation loss: 0.703370\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "115\tValidation loss: 0.702750\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "116\tValidation loss: 0.702610\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "117\tValidation loss: 0.703489\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "118\tValidation loss: 0.703601\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "119\tValidation loss: 0.703108\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "120\tValidation loss: 0.702340\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "121\tValidation loss: 0.703991\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "122\tValidation loss: 0.703505\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "123\tValidation loss: 0.702521\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "124\tValidation loss: 0.702994\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "125\tValidation loss: 0.703797\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "126\tValidation loss: 0.703726\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "127\tValidation loss: 0.703685\tBest loss: 0.702140\tAccuracy: 76.14%\n",
      "128\tValidation loss: 0.702458\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "129\tValidation loss: 0.702643\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "130\tValidation loss: 0.702828\tBest loss: 0.702140\tAccuracy: 76.13%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, n_hidden_layers=8, learning_rate=0.02, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f921a9c57b8>, total=43.6min\n",
      "[CV] n_neurons=100, n_hidden_layers=8, learning_rate=0.02, dropout_rate=0.2, batch_size=2000, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x7f921a9c57b8> \n",
      "0\tValidation loss: 0.705716\tBest loss: 0.705716\tAccuracy: 76.12%\n",
      "1\tValidation loss: 0.704911\tBest loss: 0.704911\tAccuracy: 76.12%\n",
      "2\tValidation loss: 0.703141\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "3\tValidation loss: 0.705013\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "4\tValidation loss: 0.703410\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "5\tValidation loss: 0.703838\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "6\tValidation loss: 0.704934\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "7\tValidation loss: 0.703476\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "8\tValidation loss: 0.704533\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "9\tValidation loss: 0.703627\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "10\tValidation loss: 0.704329\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "11\tValidation loss: 0.704877\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "12\tValidation loss: 0.703712\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "13\tValidation loss: 0.704300\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "14\tValidation loss: 0.703720\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "15\tValidation loss: 0.705257\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "16\tValidation loss: 0.704117\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "17\tValidation loss: 0.704289\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "18\tValidation loss: 0.704576\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "19\tValidation loss: 0.705513\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "20\tValidation loss: 0.704427\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "21\tValidation loss: 0.705001\tBest loss: 0.703141\tAccuracy: 76.12%\n",
      "22\tValidation loss: 0.702518\tBest loss: 0.702518\tAccuracy: 76.12%\n",
      "23\tValidation loss: 0.704761\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "24\tValidation loss: 0.703418\tBest loss: 0.702518\tAccuracy: 76.12%\n",
      "25\tValidation loss: 0.703611\tBest loss: 0.702518\tAccuracy: 76.12%\n",
      "26\tValidation loss: 0.703817\tBest loss: 0.702518\tAccuracy: 76.12%\n",
      "27\tValidation loss: 0.703015\tBest loss: 0.702518\tAccuracy: 76.12%\n",
      "28\tValidation loss: 0.703870\tBest loss: 0.702518\tAccuracy: 76.12%\n",
      "29\tValidation loss: 0.703358\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "30\tValidation loss: 0.702960\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "31\tValidation loss: 0.703290\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "32\tValidation loss: 0.702642\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "33\tValidation loss: 0.702633\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "34\tValidation loss: 0.704144\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "35\tValidation loss: 0.703701\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "36\tValidation loss: 0.703828\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "37\tValidation loss: 0.704817\tBest loss: 0.702518\tAccuracy: 76.14%\n",
      "38\tValidation loss: 0.703125\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "39\tValidation loss: 0.704055\tBest loss: 0.702518\tAccuracy: 76.13%\n",
      "40\tValidation loss: 0.702490\tBest loss: 0.702490\tAccuracy: 76.13%\n",
      "41\tValidation loss: 0.704921\tBest loss: 0.702490\tAccuracy: 76.13%\n",
      "42\tValidation loss: 0.702175\tBest loss: 0.702175\tAccuracy: 76.13%\n"
     ]
    }
   ],
   "source": [
    "# %%pixie_debugger\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state=17), param_distribs, n_iter=50,\n",
    "                                cv=3, random_state=17, verbose=2)\n",
    "# pdb.set_trace()\n",
    "rnd_search.fit(X_train, y_train, X_valid=X_valid, y_valid=y_valid, n_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors in previous cell:\n",
    "1. error\n",
    "row 3: unhashable type: 'numpy.ndarray'\n",
    "possible solutions:\n",
    "https://datascience.stackexchange.com/questions/39831/typeerror-unhashable-type-numpy-ndarray?rq=1\n",
    "\"The problem is that you're passing a list of numpy arrays to the mode function. It requires either a single list of values, or a single numpy array with values (basically any single container will do, but seemingly not a list of arrays).\" \n",
    "\n",
    "-- try to find where list of arrays is passed but it shouldn't be - considering this whole error message maybe there is problem with passing X_batch and y_batch values to the feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
